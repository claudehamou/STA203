---
title: "Projet STA203 : étude d'un jeu de données musical"
author: "Hamou Claude, Ray Loic"
date: "08/05/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ggplot2)
library(GGally)
library(caret)
```



Dans ce projet nous allons étudier un jeu de données musical. Ce jeu de données contient 191 variable quantitatives et 1 variables qualitative qui représente le genre de musique. 

Nous allons implémenter 3 méthodes permettant de prédire la variable de genre à partir des autres variables. Il s'agira des méthodes de régression logistique, des k plus proches voisins, et de régression ridge.
Commençons donc par importer le jeu de données.
```{r}
data=read.table("Music.txt", header = TRUE, sep=";")
#head(data)
#summary(data) #trop long
#str(data)
ncol(data) #nombre de variable
nrow(data) #nombre d'observation
```

# Partie 1: Régression logistique

## Question 1

Il y a trop de variables pour pouvoir extraire des informations en regardant les données brutes.
Mais on peut toutefois faire des analyses univariée et bivariée sur quelques variables.

Regardons tout d'abord le nombre d'individus de chaque classe.
```{r}
summary(data[192]) #nombre de d'individu de chaque groupes
```
On peut déjà remarquer que le jeu de données est relativement équilibré, ce qui nous permet de faire une étude qui ne soit pas trop biaisée.

Faisons une analyse univariée, sans considérer la variable qualitative 192.
```{r fig.height=4, fig.width=10}
boxplot(data[-192])
```
Cela ne se voit pas bien sur le graphique mais les variables 3 **PAR_SC_V** , et 179 **PAR_PEAK_RMS10FR_VAR** prennent des valeurs bien plus élevées que les autres variables. On peut notamment remarquer que ces deux variables ont des variances élevées (du fait de la répartions des points au dessus de leur boite)
Affichons un nouveau boxplot sans prendre en compte ces variables.

```{r fig.height=4, fig.width=10}
boxplot(data[-c(3,179,192)])
#boxplot((data[c(-3,-179,-192)])[2:10])
```
Nous voyons ici que les variables et 2 **PAR_SC** et 178 **PAR_PEAK_RMS10FR_MEAN** prennent elles aussi des valeurs bien supérieures aux autres variables, avec là encore une grande variance entre les valeurs. Par ailleurs cela n'est pas étonnant car ces variables et les variables précédentes sont reliées. En effet les variables que nous avons ici représentent des moyennes, et les variables précédentes représentaient la variance associée. 

Faisons un dernier boxplot sans ces variables.  


```{r fig.height=4, fig.width=10}
boxplot(data[-c(2,3,178,179,192)])
#boxplot((data[-c(2,3,178,179,192)])[175:180])
```


Interessons nous maintenant aux corrélations entre les variable, en faisant une étude bivariée du jeu de données.
Pour cela calculons et affichons la matrice de corrélations.

```{r fig.height=4}
matrice_data=data.matrix(data)
correlation_data=cor(matrice_data)
ggcorr(matrice_data,nbreaks = 4, palette = "RdGy")
#corrplot(correlation_data, tl.pos='n')
```
Comme cela était attendu, le graphique est quasiment illisible. Mais on parvient tout de même à discerner des zones de forte covariance.
Implementons une fonction qui affiche les variables dont la covariance est comprise entre 2 bornes, afin de retirer des informations plus pertinentes de la matrice de corrélation.
```{r}
print_corr_borne= function(mat_cor,seuil_min,seuil_max){
  l=nrow(mat_cor)
  found=FALSE
  for(i in 1:l){
    for(j in 1:i){
      if(mat_cor[i,j]>seuil_min &&mat_cor[i,j]<seuil_max){
        #Affiche le nom des variables correspondantes
        found=TRUE
        print(names(data)[c(i,j)])
      }
    }
  }
  if(!found){
    print("Il n'y a aucune covariance n'est comprise entre ces bornes")
  }
}
```

Nous pouvons alors afficher les variables trés corrélées, dont la covariance se trouve dans $\big]0.99;1\big[$
```{r}
print_corr_borne(correlation_data,0.99,1)
```

Ainsi que les variables trés anti-corrélées, dont la covariance se trouve dans $\big]-1;0.99\big[$
```{r}
print_corr_borne(correlation_data,-1,-0.99)
```

On remarque donc que les variables très corrélées sont de type *MFCCV* et *MFCC* ainsi que des variables *ASE*.





Considérons les variables 128 à 147 et 148 à 167. En regardant le jeu de données et son descriptif, il semblerait que ces deux groupes de variables soient égaux.
Pour le confirmer on écrit un script qui renvoie le nombre de différence entre ces 2 groupes.

```{r}
#Egalite 128:147 et 148:167
dif=0
for(i in 128:147){
  dif=sum(data[i]!=data[i+20])
}
dif
```

Comme indiqué dans le descriptif du dataset, les colonnes 128 à 147 et 148 à 167 ont les mêmes valeurs. On ne considèrera donc pas les colonnes 148 à 167 dans la suite.



Les données **PAR_ASE_M**, **PAR_ASE_MV**, **PAR_SFM_M** et **PAR_SFM_MV** représentent les moyennes des variables 4 à 37, 39 à 72, 78 à 101, et 103 à 126. Pour réduire le nombre de variable il peut être préférable dans un premier temps de pas considéré les colonnes 4 à 37, 39 à 72, 78 à 101, et 103 à 126 comme les variables **PAR_ASE_M**, **PAR_ASE_MV**, **PAR_SFM_M** et **PAR_SFM_MV** en sont des agréats.



On réalise les opérations de nettoyage précédement expliquées et on note *X* le nouveau data frame de données que nous allons utiliser dans la suite.
Et *Y* le vecteur contenant la variable qualitative **GENRE** en binaire, avec $Classical = 0$ et $Jazz = 1$.

```{r}
#Colonnes que nous n'utiliserons pas dans la suite
del=c(148:167,
      4:37,
      39:72,
      78:101,
      103:126)
#
X=data[,-c(del,192)]
#log des variables PAR_SC_V et PAR_ASC_V
X["PAR_SC_V"]=log(data["PAR_SC_V"])
X["PAR_ASC_V"]=log(data["PAR_ASC_V"])

#
GENRE=data[,192]
Y=1*(GENRE=="Jazz")
```



Nous cherchons à déterminer un modèle logistique permettant de d'estimer les valeurs de la variable **Y**.
Cette variable prend deux valeurs **0** (s'il s'agit du genre classique) et **1** (s'il s'agit du genre jazz). C'est donc la variable binaire **Y** que nous cherchons à expliquer. Les valeurs des covariables $x_i$, qui représentent les autres paramètres, sont différentes pour chaque segment de morceau musical, on modélise l'expérience comme la réalisation de $n=6447$ variables aléatoires indépendantes de Bernouilli $Z_i$, de paramètre $\pi(x_i) = \mathbb{P}(Z_i = 1|x_i)$. Dans notre cas, on a donc $Z_i \text{ ~ } \mathcal{B}(1,\pi(x_i))$. Ici, une régression linéaire ne serait pas adaptée (à cause de la contrainte sur $\pi$). En revanche, on peut définir la fonction de lien *logit* telle que $$\text{logit}(\pi(x_i)) = \text{log}(\frac{\pi(x_i)}{1 - \pi(x_i)})$$. Notre modèle est alors tel que $$\text{logit}(\pi(x_i)) = x_i  \theta $$, régresseur linéaire des covariables.




## Question 2

On utilise le code proposé pour générer des data-frames de *training* permettant de fitter le modèle et des data-frames de *test*.

```{r}
set.seed(103)
n=nrow(data)
train=sample(c(TRUE,FALSE),n,rep=TRUE,prob=c(2/3,1/3))
X_training=X[train,]
X_test=X[!train,]

GENRE_training=GENRE[train]
GENRE_test=GENRE[!train]

Y_training=Y[train]
Y_test=Y[!train]
```



## Question 3

### Mod0 

```{r}
indices=c("PAR_TC","PAR_SC", "PAR_SC_V", "PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV")
Mod0=glm(Y_training~.,family=binomial,data=X_training[indices])
summary(Mod0)
#par(mfrow=c(2,2)) 
#plot(Mod0)
Mod0.predict_test=predict(Mod0, newdata=X_test[indices], type="response")
```

### ModT

```{r}
ModT=glm(Y_training~.,family=binomial,data=X_training)
summary(ModT)
#par(mfrow=c(2,2))
#plot(ModT)
ModT.predict_test=predict(ModT, newdata=X_test, type="response")
```

### Mod1

```{r}
#Pr(>|t|)  de ModT
ModT.p_val=coef(summary(ModT))[,4]
#On cherche les indices de p-value < 5%
indice_sign_5=names(which(ModT.p_val<0.05))
#Prediction de Mod1
Mod1=glm(Y_training~.,family=binomial,data=X_training[indice_sign_5])
summary(Mod1)
#par(mfrow=c(2,2))
#plot(Mod1)
Mod1.predict_test=predict(Mod1, newdata=X_test[indice_sign_5], type="response")
```

### Mod2 
```{r}
#Pr(>|t|)  de ModT
ModT.p_val=coef(summary(ModT))[,4]
#On cherche les indices de p-value < 20%
indice_sign_20=names(which(ModT.p_val<0.2))[c(-1)] #car 1er element est l'intercept
#Prediction de Mod1
Mod2=glm(Y_training~.,family=binomial,data=X_training[indice_sign_20])
summary(Mod2)
#par(mfrow=c(2,2))
#plot(Mod2)
Mod2.predict_test=predict(Mod2, newdata=X_test[indice_sign_20], type="response")
```

### ModAIC

```{r warning=FALSE}
library(MASS)
ModAIC=stepAIC(ModT, direction = "both", trace = FALSE)

summary(ModAIC)
#par(mfrow=c(2,2))
#plot(ModAIC)
ModAIC.predict_test=predict(ModAIC, newdata=X_test, type="response")
```




## Question: 4

Dans un premier temps, traçons la courbe ROC du modèle **ModT** ainsi que les courbes des règles aléatoires et parfaites.
Pour cela on récupère la liste des prédictions du modèle sur les données de *training*, grace à la fonction **predict**. Et on les stocke dans la variable **ModT.predict_training**. Pour rappel, on a généré les prédictions sur les données *test* dans la partie précédente.

Ensuite on utilise la fonction **prediction** du package **ROCR** pour comparer nos prédictions aux valeurs réelles de **Y_test**.

```{r fig.height=5, fig.width=5}
ModT.predict_training=predict(ModT,type="response") #predition de ModT sur les données training
library(ROCR)
p_training=prediction(ModT.predict_training,Y_training)
p_test=prediction(ModT.predict_test,Y_test)
```

On peut alors utiliser la fonction **performance** avec les attributs *sens* et *fpr* pour calculer la courbe de ROC des données training et test, et afficher le tout dans un graphique.
On ajoute à ce graphique la règle aléatoire (première bissectrice) et la règle parfaite qui prédit 100% des vrais positifs et 0% des faux positif (segment $(0,0)-(0,1)$ et $(1,0)-(1,1)$)


```{r fig.height=5, fig.width=5}
#plot ROC des données training
plot(performance(p_training,"sens","fpr"),col=2,lwd=3,add=FALSE)
#plot ROC des données test
plot(performance(p_test,"sens","fpr"),col=3,lwd=3,add=TRUE)
# règle aléatoire
lines(c(0,1),c(0,1),col=1,lty=1,lwd=2)      
# règle parfaite
segments(c(0,0),c(0,1),c(0,1),c(1,1),col=4,lty=4,lwd=2) 

title("ROC sur les données de training \n et de test pour ModT")
legend("bottomright", 
       legend=c("ROC training",
                "ROC test", "Règle aléatoire",
                "Règle parfaite"),
       col=c(2,3,1,4), lty=c(1,1,1,2))
```

On remarque que les courbes ROC des données *training* et *test* se superposent. Donc le modèle se comporte de la même manière sur les données de *test* que sur les données de *training*. 


Traçons maintenant les courbes ROC de tous les modèles de la question 3, pour des prédictions sur les données de test. Cela va nous permettre de comparer les modèles entre eux. On procède de la même manière que précédement pour généré les courbes ROC. Et on stock la sortie de la fonction **prediction** dans le variables **.ROC_pred_test**.
Pour une raison de clarté on appel aussi la fonction sur ModT, même si on l'a déjà fait au début de cette question.

```{r fig.height=5, fig.width=5}
Mod0.ROC_pred_test=prediction(Mod0.predict_test,Y_test)
ModT.ROC_pred_test=prediction(ModT.predict_test,Y_test)
Mod1.ROC_pred_test=prediction(Mod1.predict_test,Y_test)
Mod2.ROC_pred_test=prediction(Mod2.predict_test,Y_test)
ModAIC.ROC_pred_test=prediction(ModAIC.predict_test,Y_test)
```

Pour pouvoir comparer les modèles entre eux il est plus pertinent de comparer les airessous les courbes ROC. Calculons donc ces aires pour nos cinq modèles. Pour ce faire on utilise la fonction **performance** avec l'attribut *auc*. On stock la sortie de cette fonction dans les variables **.perf_AUC**. La valeur de l'aire sous la courbe ROC se trouve alors dans l'attribut *y.values* de ces variable (qui est une liste). On arrondi cette valeur à 3 décimales, et on la stock dans les variables **.AUC**.

```{r fig.height=5, fig.width=5}
Mod0.perf_AUC = performance(Mod0.ROC_pred_test, "auc")
ModT.perf_AUC = performance(ModT.ROC_pred_test, "auc")
Mod1.perf_AUC = performance(Mod1.ROC_pred_test, "auc")
Mod2.perf_AUC = performance(Mod2.ROC_pred_test, "auc")
ModAIC.perf_AUC = performance(ModAIC.ROC_pred_test, "auc")

#Recuperation valeur aire
Mod0.AUC=round(Mod0.perf_AUC@y.values[[1]],3) 
ModT.AUC=round(ModT.perf_AUC@y.values[[1]],3) 
Mod1.AUC=round(Mod1.perf_AUC@y.values[[1]],3) 
Mod2.AUC=round(Mod2.perf_AUC@y.values[[1]],3) 
ModAIC.AUC=round(ModAIC.perf_AUC@y.values[[1]],3)
```

On peut alors tracer un graphique contenant les ROC de nos cinq modèles. On affiche la valeur de l'aire sous la courbe de ROC de chaque modèle dans la légende. 
```{r fig.height=5, fig.width=5}
#Plot tous les ROC
plot(performance(Mod0.ROC_pred_test,"sens","fpr"),col=2,lwd=3,add=FALSE)
plot(performance(ModT.ROC_pred_test,"sens","fpr"),col=3,lwd=3,lty=1,add=TRUE)
plot(performance(Mod1.ROC_pred_test,"sens","fpr"),col=4,lwd=3,lty=2,add=TRUE)
plot(performance(Mod2.ROC_pred_test,"sens","fpr"),col=5,lwd=3,lty=3,add=TRUE)
plot(performance(ModAIC.ROC_pred_test,"sens","fpr"),col=6,lwd=3,lty=4,add=TRUE)
lines(c(0,1),c(0,1),col=1,lty=1,lwd=1)


#Titre et legende avec valeur aire
title("ROC sur les données de test")
legend("bottomright", 
       legend=c(paste("Mod0: ", Mod0.AUC),
                paste("ModT: ", ModT.AUC),
                paste("Mod1: ", Mod1.AUC), 
                paste("Mod2: ", Mod2.AUC), 
                paste("ModAIC: ", ModAIC.AUC)), 
       col=c(2,3,4,5,6), lty=c(1,1,2,3,4))
```

On peut remarquer que les trois derniers modèles se superposent et que par conséquent leur aires sont extrement proche. Il est donc difficile à ce stade de dire quel est le meilleur modèle.

## Question 5

Cherchons maintenant quel est le meilleur modèle parmi ceux calculer précédement.

On commence par le regarder l'erreur quadratique moyenne de prévision de chaque modèle sur les données de **training**. 
Les prédictions sur les données d'entrainement sont contenues dans l'attribut **fitted.values**
```{r}
#Erreur 
mean((Y_training-Mod0$fitted.values)^2)
mean((Y_training-ModT$fitted.values)^2)
mean((Y_training-Mod1$fitted.values)^2)
mean((Y_training-Mod2$fitted.values)^2)
mean((Y_training-ModAIC$fitted.values)^2)
```

Regardons aussi l'erreur de classification des modèles. Les modèles étant des fonctionS logistiqueS, ils renvoient donc des prédictions continuent dans $\big]0,1\big[$. On seuil les prédiction à *0.5*.

```{r}
#prediction seuillee
Mod0.class_train=ifelse(Mod0$fitted.values>0.5,1,0)  
ModT.class_train=ifelse(ModT$fitted.values>0.5,1,0)
Mod1.class_train=ifelse(Mod1$fitted.values>0.5,1,0)
Mod2.class_train=ifelse(Mod2$fitted.values>0.5,1,0)
ModAIC.class_train=ifelse(ModAIC$fitted.values>0.5,1,0)

#Erreur de classification
mean(Mod0.class_train!=Y_training)
mean(ModT.class_train!=Y_training)
mean(Mod1.class_train!=Y_training)
mean(Mod2.class_train!=Y_training)
mean(ModAIC.class_train!=Y_training)
```
Considèrons ensuite ces mêmes erreurs sur les prédiction de **test**.
```{r}
#Erreur 
mean((Y_test-Mod0.predict_test)^2)
mean((Y_test-ModT.predict_test)^2)
mean((Y_test-Mod1.predict_test)^2)
mean((Y_test-Mod2.predict_test)^2)
mean((Y_test-ModAIC.predict_test)^2)

#prediction seuillée
Mod0.class_test=ifelse(Mod0.predict_test>0.5,1,0)  
ModT.class_test=ifelse(ModT.predict_test>0.5,1,0)
Mod1.class_test=ifelse(Mod1.predict_test>0.5,1,0)
Mod2.class_test=ifelse(Mod2.predict_test>0.5,1,0)
ModAIC.class_test=ifelse(ModAIC.predict_test>0.5,1,0)


mean(Mod0.class_test!=Y_test)
mean(ModT.class_test!=Y_test)
mean(Mod1.class_test!=Y_test)
mean(Mod2.class_test!=Y_test)
mean(ModAIC.class_test!=Y_test)
```

Le modèle qui minimise toutes ces erreurs est le modèle **ModT**. Toutefois les valeurs des modèles **Mod2**, et **ModAIC** sont encore très proches des valeurs de **ModT**. Regardons donc l'AIC pour trancher entre ces modèles.

```{r}
Mod0$aic
ModT$aic
Mod1$aic
Mod2$aic
ModAIC$aic
```

**ModAIC** minimise l'AIC, nous choissons donc ce modèle parmi ceux que nous avons présenté.

Nos données étant individuelles, nous pouvons faire un test d'adéquation, en utilisant le *test de Hosmer et Lemeshow*.


# Partie 2: K plus proches voisins (K-NN)

## Question 1: Principe du K-NN

La méthode des k plus proches voisins (knn ou k nearest neighbours) est une méthode simple à utiliser dans des cas de classification ou régression. Il s'agit d'une méthode non paramétrique dans laquelle le modèle mémorise les observations de l'ensemble d'apprentissage et s'en sert pour les observations des données de test. Elle consiste à fixer un nombre k de voisins des nouvelles données d'entrée, de séléctionner les k plus proches (en fonction d'une certaine distance, la distance euclidienne par exemple) et de conserver la classe correspondant à celle majoritairement représentée parmi les différents voisins retenus.

Pour choisir le meilleur k, il faut tester différentes valeurs et retenir celle qui minimise le taux d'erreur de l'ensemble de test.

## Question 2: Implémentation K-NN

En premier lieu on effectue la méthode pour $$K=1$$ voisin.
```{r}
library(class)
pred_knn_test <-knn(X_training,X_test, Y_training, k=1) # tout d'abord on teste avec k=1
summary(pred_knn_test)
err_test_1=sum(pred_knn_test!=Y_test)/length(Y_test)
     
```
On fait maintenant une boucle qui permet de calculer cette méthode pour $$k$$ allant de 1 à 200 voisins et on retient à chaque fois les erreurs d'apprentissage et d'entraînement.

```{r eval=FALSE, include=TRUE}
err_test = rep(NA,length=150)
err_train = rep(NA, length=150)
for (k in 1:200){
  mod_train <-knn(X_training,X_training,Y_training,k=k)
  pred_knn <-knn(X_training,X_test, Y_training, k=k)
  err_train[k] = mean(mod_train!=Y_training)
  err_test[k] = mean(pred_knn!=Y_test)}

save(mod_train,file = "mod_train_sta203.RData")
save(pred_knn,file = "pred_knn_sta203.RData")
save(err_test,file = "err_test_sta203.RData")
save(err_train,file = "err_train_sta203.RData")
```

Pour ne pas avoir à faire le calcul lors de la compilation du rapport, nous avons enregistré les données.
On affiche maintenant la courbe des deux erreurs en fonction du nombre de voisins retenus.
```{r}

load("mod_train_sta203.RData")
load("pred_knn_sta203.RData")
load("err_test_sta203.RData")
load("err_train_sta203.RData")
K = which.min(err_test)
K # c'est toujours le cas où k=1 qui minimise le taux d'erreur de test
err_test[(-1)]
vec_k = 1:200
plot(vec_k, err_test, type="b", col="blue", xlab="nombre de voisins",ylab=" erreurs train et test", pch=20, 
     ylim=range(c(err_test, err_train))) 
lines(vec_k, err_train,type="b",col="red",pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
```
## Question 3 : Quelle conclusion en tirer ?

En regardant uniquement l'erreur d'apprentissage, c'est le modèle le plus complexe qui est désigné, c'est-à-dire celui de $$k=1$$. Il s'agit du phénomène de sur-apprentissage. On veut vérifier que ce soit est adapté en regardant l'erreur de test (courbe bleue). Celle-ci pointe aussi le même modèle mais cette vérification confirme qu'ici le sur-apprentissage est le meilleur choix de méthode.


# Partie 3: Régression Ridge

## Question 1: Interêt de la régression ridge

Dans le cadre du modèle binomial qui est le nôtre, la régression ridge est un type de régression logistique pénalisée, qui correspond donc à l'optimisation d'un critère. L'ajustement des paramètres de pénalisation permet de choisir un modèle plus performant qu'une "simple" régression logistique.

## Question 2 

Les deux cas extrêmes représentent les valeurs extrêmes pour $\lambda$ qui permet la pénalisation. Pour un $\lambda$ élevé comme $10^{10}$, le critère est fortement pénalisé, on se rapproche du cas i.i.d. Pour un $\lambda$ faible comme $10^{-2}$ on est dans le cas inverse où le critère est peu pénalisé, ce qui correspond presque à une régression logistique habituelle.

```{r}
library(glmnet)
les_lambda=10^seq(10,-2,length=100) 
#model_matrix=model.matrix(GENRE~.,data)[,-1]
model_matrix=model.matrix(Y~.,X)[,-1]

ridge=glmnet(model_matrix[train,],Y_training,family = c("binomial"),alpha=0,lambda=les_lambda) #alpha=0=ridge

plot(ridge,xvar="lambda") #coef vs log(lambda)        
plot(ridge) # norme L1


ridge_predict=predict(ridge, newx = model_matrix[!train,], s = 10^5)
```

## Question 3

La régression ridge implique d'omptimiser un hyperparmétre **lambda**. La fonction **cv.glmnet** permet de faire cette optimisation automatiquement, en appliquant un algorithme de validation croisée afin de comparer un modèle ridge pour différentes valeurs de lambda. Cette valeur optimale sera le **lambda** qui minimise la courbe affichée lorsque l'on appelle la fonction plot sur la sortie de **cv.glmnet**.

```{r}
set.seed(314)
(cv=cv.glmnet(model_matrix[train,],Y_training,family = c("binomial"),alpha=0,nfolds = 10))#alpha=0=ridge
plot(cv)
(best_lambda=cv$lambda.min) #meilleur lambda
```



```{r}
ridge_pred_bestl=predict(ridge,s=cv$lambda.min ,newx=model_matrix[!train,])
mean((ridge_pred_bestl-Y_test)^2)                      

plot(performance(prediction(ridge_pred_bestl,Y_test),"sens","fpr"),col=2,lwd=3,lty=1,add=FALSE)

```


## Question 4

On reprend la question 3 en utilisant toutes les variables du data-frame **data** (excepté la variable GENRE).


```{r}
set.seed(4658)
model_matrix=model.matrix(Y~.,data[-c(192)])[,-1]
(cv_all=cv.glmnet(model_matrix[train,],Y_training,family = c("binomial"),alpha=0,nfolds = 10))#alpha=0=ridge
plot(cv_all)
(best_lambda=cv_all$lambda.min) #meilleur lambda
```



```{r}
ridge_all_pred_bestl=predict(cv_all,s=cv_all$lambda.min ,newx=model_matrix[!train,])
mean((ridge_all_pred_bestl-Y_test)^2)                      

plot(performance(prediction(ridge_all_pred_bestl,Y_test),"sens","fpr"),col=3,lwd=3,lty=1,add=FALSE)
plot(performance(prediction(ridge_pred_bestl,Y_test),"sens","fpr"),col=2,lwd=3,lty=1,add=TRUE)
title("ROC sur les modèles ridge")
legend("bottomright", 
       legend=c("ROC Modèle contenant toutes les variables",
                "ROC Modèle contenant les variables selectionnées en Partie 1"),
       col=c(3,2), lty=c(1,1))
```

La régression ridge maximisant l'aire sous la courbe est celle contenant toutes les variables, nous allons donc choisir ce modèle. Toutefois, il faut noter que les valeurs de la prédiction ne sont pas entre 0 et 1.



# Conclusion

Comparons les 3 modèles selectionnés dans les partie précedentes. Pour rappel les modèles selectionnés sont:

- Partie 1: **ModAIC**
- Partie 2: **K-nn avec K=1**
- Partie 3: **Ridge sur toutes les variables ridge_all_pred_bestl**

Cherchons le modèle dont l'erreur est la plus faible.
Commençons par l'erreur quadratique sur les données de test.
```{r}
mean((Y_test-ModAIC.predict_test)^2)
mean((Y_test-ridge_all_pred_bestl)^2)
```
On remarque que l'erreur de la regression ridge est très élevée, cela est du au fait que les valeurs des prédictions ne sont pas entre 0 et 1.

Seuillons les données à $0.5$ et comparons les erreurs de classifications, ainsi que les matrices de confusions.

```{r}
mean(ModAIC.class_test!=Y_test)
mean(pred_knn_test!=Y_test)
mean(ifelse(ridge_all_pred_bestl>0.5,1,0)!=Y_test)
```

```{r}
table(ModAIC.class_test,Y_test)
table(pred_knn_test,Y_test)
table(ifelse(ridge_all_pred_bestl>0.5,1,0),Y_test)
```

Le modèle avec la plus faible erreur de classification est donc le modèle ridge contenant toutes les variables du jeux de données. On remarque qu'il est principalement sensible aux faux positifs.

Pour tester la performance de généralisation du modèle il aurait fallu disiser notre jeu de données en 3 parties (aprentissage, validation, test) avant de faire les analyses. Ici notre échantillons a été utilisé comme un échantillon de validation, il n'est donc pas possible de procèder à un test de performance de généralisation telquel. Mais on peut toutefois utiliser l' estimation de la performance sur échantillon indépendant (partie 10.2.4) pour générer un estimateur de l'erreur quadratique moyenne de prédiction.
```{r}
mean((Y_test-ifelse(ridge_all_pred_bestl>0.5,1,0))^2)
```

On aurait pu aussi essayer d'autres méthodes de classification comme celle des forêts aléatoires ou des arbres CART.


# Bonus

Implémentons un arbre de décision et une forêt aléatoire.

## Arbre de décision

Commençons par un arbre décision élagué.
```{r}
library(rpart)
library(rpart.plot)
arbre = rpart(GENRE_training~., data=X_training)
arbre$cptable
arbre = prune(arbre, cp = arbre$cptable[which.min(arbre$cptable[,"xerror"]),"CP"])
rpart.plot(arbre)
```

La prédiction sur les données de test nous donne:

```{r}
arbre_predict = predict(arbre, X_test,type="class")
table(arbre_predict, GENRE_test)
```


## Forêt aléatoire

Implémentons maintenant une forêt aléatoire de 50 arbres.
```{r}
library(randomForest)
foret=randomForest(GENRE_training~.,data=X_training, ntree = 50, 
                   mtry = 2, na.action = na.roughfix)
foret_predict = predict(foret, X_test, type="class")
table(foret_predict, GENRE_test)
```


## Erreur de classification des données de test
Regradons l'erreur moyenne sur les données de prédictions de ces deux méthodes.
```{r}
mean(arbre_predict!=GENRE_test)
mean(foret_predict!=GENRE_test)
```

On remarque donc que la méthode de forêt aléatoire permet d'avoir une meilleur classification sur notre jeu de données que la méthode ridge. Même si les résultats restent proches.
