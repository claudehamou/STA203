---
title: "Projet STA203"
author: "Hamou Claude, Ray Loic"
date: "25/04/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ggplot2)
library(GGally)
library(caret)
```



Dans ce projet nous allons étudier un jeu de données musical. Ce jeu de données contient 191 variable quantitatives et 1 variables qualitative qui représente le genre de musique. Nous allons implémenter 3 méthodes permettant de prédire la variable de genre à partir des autres variables.
Commençons donc par importer le jeu de données.
```{r}
data=read.table("Music.txt", header = TRUE, sep=";")
#head(data)
#summary(data) #trop long
#str(data)
ncol(data) #nombre de variable
nrow(data) #nombre d'observation
```

# Partie 1

## Question 1

Il y a trop de variables pour pouvoir extraire des informations en regardant les données bruts.
Mais on peut toutefois faire des analyses univariée et bivariée sur quelques variables.

Regardons tout d'abord le nombre d'individu de chaque classe.
```{r}
summary(data[192]) #nombre de d'individu de chaque groupes
```
On peut déjà remarquer que le jeu de données est relativement équilibrés, ce qui nous permet de faire une étude qui ne soit pas trop biaisée.

Faisons une analyse univariée, sans considérer la variable qualitative 192.
```{r fig.height=4, fig.width=10}
boxplot(data[-192])
```
Cela ne ce voit pas bien sur le graphique mais les variables 3 **PAR_SC_V** , et 179 **PAR_PEAK_RMS10FR_VAR** prennent des valeurs bien plus élevées que les autres variables. On peut notamment remarquer que ces deux variables ont des variances élevées (du fait de la répartions des points au dessus de leur boites)
Affichons un nouveau boxplot sans prendre en compte ces variables.

```{r fig.height=4, fig.width=10}
boxplot(data[-c(3,179,192)])
#boxplot((data[c(-3,-179,-192)])[2:10])
```
Nous voyons ici que les variables et 2 **PAR_SC** et 178 **PAR_PEAK_RMS10FR_MEAN** prennent elles aussi des valeurs bien supérieures aux autres variables, avec là encore une grande variance entre les valeurs. Par ailleurs cela n'est pas étonnant car ces variables et les variables précédentes sont reliées. En effet les variables que nous avons ici représente des moyennes, et les variables précédetent représentaient la variance associée. 

Faisons un dernier boxplot sans ces variables.  

**ON POURRAIT FAIRE UN TRUC MIEUX EN ECRIVANT UNE FONCTION QUI RENVOIE LE NOMS DE TOUTES LES VARIAVLES QUI PRENNENT DES VALEURS SUPERIEURES A UNE CONSTANT GENRE 1**

```{r fig.height=4, fig.width=10}
boxplot(data[-c(2,3,178,179,192)])
#boxplot((data[-c(2,3,178,179,192)])[175:180])
```


Interessons nous maintenant aux corrélations entre les variable, en faisant une étude bivariée du jeu de données.
Pour cela calculons et affichons la matrice de corrélations.

```{r fig.height=4}
matrice_data=data.matrix(data)
correlation_data=cor(matrice_data)
ggcorr(matrice_data,nbreaks = 4, palette = "RdGy")
#corrplot(correlation_data, tl.pos='n')
```
Comme cela était attendu, le graphique est quasiment illisible. Mais on parvient tout de même à diserner des zones de forte covariance.
Implementons une fonction qui affiche les variables dont la covariance est comprise entre 2 bornes, afin de retirer des informations plus pertinentes de la matrice de corrélation.
```{r}
print_corr_borne= function(mat_cor,seuil_min,seuil_max){
  l=nrow(mat_cor)
  found=FALSE
  for(i in 1:l){
    for(j in 1:i){
      if(mat_cor[i,j]>seuil_min &&mat_cor[i,j]<seuil_max){
        #Affiche le nom des variables correspondantes
        found=TRUE
        print(names(data)[c(i,j)])
      }
    }
  }
  if(!found){
    print("Il n'y a aucune covariance n'est comprise entre ces bornes")
  }
}
```

Nous pouvons alors afficher les variables trés corrélées, dont la covariance se trouve dans $\big]0.99;1\big[$
```{r}
print_corr_borne(correlation_data,0.99,1)
```

Ainsi que les variables trés anti-corrélées, dont la covariance se trouve dans $\big]-1;0.99\big[$
```{r}
print_corr_borne(correlation_data,-1,-0.99)
```

On remarque donc que les variables très corrélées sont de type *MFCCV* et *MFCC* ainsi que des variables *ASE*.





Considérons les variables 128 à 147 et 148 à 167. En regardant le jeu de données et son déscriptif, il semblerait que ces deux groupes de variables soient égaux.
Pour le confirmer on écrit un script qui renvoie le nombre de différence entre ces 2 groupes.

```{r}
#Egalite 128:147 et 148:167
dif=0
for(i in 128:147){
  dif=sum(data[i]!=data[i+20])
}
dif
```

Comme indiqué dans le descriptif du dataset, les colonnes 128 à 147 et 148 à 167 ont les mêmes valeurs. On ne considèrera donc pas les colonnes 148 à 167 dans la suite.



Les données **PAR_ASE_M**, **PAR_ASE_MV**, **PAR_SFM_M** et **PAR_SFM_MV** représentent les moyennes des variables 4 à 37, 39 à 72, 78 à 101, et 103 à 126. Pour réduire le nombre de variable il peut être préférable dans un premier temps de pas considéré les colonnes 4 à 37, 39 à 72, 78 à 101, et 103 à 126 comme les variables **PAR_ASE_M**, **PAR_ASE_MV**, **PAR_SFM_M** et **PAR_SFM_MV** en sont des agréats.



On réalise les opérations de nettoyage précédement expliquées et on note *X* le nouveau data frame de données que nous allons utiliser dans la suite.
Et *Y* le vecteur contenant la variable qualitative **GENRE** en binaire, avec $Classical = 0$ et $Jazz = 1$.

```{r}
#Colonnes que nous n'utiliserons pas dans la suite
del=c(148:167,
      4:37,
      39:72,
      78:101,
      103:126)
#
X=data[,-c(del,192)]
#log des variables PAR_SC_V et PAR_ASC_V
X["PAR_SC_V"]=log(data["PAR_SC_V"])
X["PAR_ASC_V"]=log(data["PAR_ASC_V"])

#
GENRE=data[,192]
Y=1*(GENRE=="Jazz")
```



Nous cherchons à déterminer un modèle logistique permettant de d'estimer les valeurs de la variable **Y**.
Cette variable prend deux valeurs **0** (s'il s'agit du genre classique) et **1** (s'il s'agit du genre jazz). C'est donc la variable binaire **Y** que nous cherchons à expliquer. Les valeurs des covariables $x_i$, qui représentent les autres paramètres, sont différentes pour chaque segment de morceau musical, on modélise l'expérience comme la réalisation de $n=6447$ variables aléatoires indépendantes de Bernouilli $Z_i$, de paramètre $\pi(x_i) = \mathbb{P}(Z_i = 1|x_i)$. Dans notre cas, on a donc $Z_i \text{ ~ } \mathcal{B}(1,\pi(x_i))$. Ici, une régression linéaire ne serait pas adaptée (à cause de la contrainte sur $\pi$). En revanche, on peut définir la fonction de lien *logit* telle que $$\text{logit}(\pi(x_i)) = \text{log}(\frac{\pi(x_i)}{1 - \pi(x_i)})$$. Notre modèle est alors tel que $$\text{logit}(\pi(x_i)) = x_i  \theta $$, régresseur linéaire des covariables.




## Question 2

On utilise le code proposé pour générer des data-frame de *training* permettant de fitter le modèle et des data-frame de *test*.

```{r}
set.seed(103)
n=nrow(data)
train=sample(c(TRUE,FALSE),n,rep=TRUE,prob=c(2/3,1/3))
X_training=X[train,]
X_test=X[!train,]

GENRE_training=GENRE[train]
GENRE_test=GENRE[!train]

Y_training=Y[train]
Y_test=Y[!train]
```



## Question 3

### Mod0 

```{r}
indices=c("PAR_TC","PAR_SC", "PAR_SC_V", "PAR_ASE_M", "PAR_ASE_MV", "PAR_SFM_M", "PAR_SFM_MV")
Mod0=glm(Y_training~.,family=binomial,data=X_training[indices])
summary(Mod0)
par(mfrow=c(2,2)) 
plot(Mod0)
Mod0.predict_test=predict(Mod0, newdata=X_test[indices], type="response")
```

### ModT

```{r}
ModT=glm(Y_training~.,family=binomial,data=X_training)
summary(ModT)
par(mfrow=c(2,2))
plot(ModT)
ModT.predict_test=predict(ModT, newdata=X_test, type="response")
```

### Mod1

```{r}
#Pr(>|t|)  de ModT
ModT.p_val=coef(summary(ModT))[,4]
#On cherche les indices de p-value < 5%
indice_sign_5=names(which(ModT.p_val<0.05))
#Prediction de Mod1
Mod1=glm(Y_training~.,family=binomial,data=X_training[indice_sign_5])
summary(Mod1)
par(mfrow=c(2,2))
plot(Mod1)
Mod1.predict_test=predict(Mod1, newdata=X_test[indice_sign_5], type="response")
```

### Mod2 
```{r}
#Pr(>|t|)  de ModT
ModT.p_val=coef(summary(ModT))[,4]
#On cherche les indices de p-value < 20%
indice_sign_20=names(which(ModT.p_val<0.2))[c(-1)] #car 1er element est l'intercept
#Prediction de Mod1
Mod2=glm(Y_training~.,family=binomial,data=X_training[indice_sign_20])
summary(Mod2)
par(mfrow=c(2,2))
plot(Mod2)
Mod2.predict_test=predict(Mod2, newdata=X_test[indice_sign_20], type="response")
```

### ModAIC

**C'est peut etre mieux d'utiliser toutes les variabes du data frame**

```{r warning=FALSE}
library(MASS)
ModAIC=stepAIC(ModT, direction = "both", trace = FALSE)

summary(ModAIC)
par(mfrow=c(2,2))
plot(ModAIC)
ModAIC.predict_test=predict(ModAIC, newdata=X_test, type="response")
```




## Question: 4

Dans un premier temps, traçons la courbe ROC du modèle **ModT** ainsi que les courbes des règles aléatoires et parfaites.
Pour cela on récupère la liste des prédictions du modèle sur les données de *training*, grace à la fonction **predict**. Et on les stock dans la variable **ModT.predict_training**. Pour rappel, on a généré les prédictions sur les données *test* dans la partie précédente.

Ensuite on utilise la fonction **prediction** du package **ROCR** pour comparer nos prédictions aux valeurs réelles de **Y_test**.

```{r fig.height=5, fig.width=5}
ModT.predict_training=predict(ModT,type="response") #predition de ModT sur les données training
library(ROCR)
p_training=prediction(ModT.predict_training,Y_training)
p_test=prediction(ModT.predict_test,Y_test)
```

On peut alors utiliser la fonction **performance** avec les attributs *sens* et *fpr* pour calculer la courbe de ROC des données training et test, et afficher le tout dans un graphique.
On ajoute à ce graphique la règle aléatoire (première bissectrice) et la règle parfaite qui prédit 100% des vrais positifs et 0% des faux positif (segment $(0,0)-(0,1)$ et $(1,0)-(1,1)$)


```{r fig.height=5, fig.width=5}
#plot ROC des données training
plot(performance(p_training,"sens","fpr"),col=2,lwd=3,add=FALSE)
#plot ROC des données test
plot(performance(p_test,"sens","fpr"),col=3,lwd=3,add=TRUE)
# règle aléatoire
lines(c(0,1),c(0,1),col=1,lty=1,lwd=2)      
# règle parfaite
segments(c(0,0),c(0,1),c(0,1),c(1,1),col=4,lty=4,lwd=2) 

title("ROC sur les données de training \n et de test pour ModT")
legend("bottomright", legend=c("ROC training", "ROC test", "Règle aléatoire", "Règle parfaite"), col=c(2,3,1,4), lty=c(1,1,1,2))
```

On remarque que les courbes ROC des données *training* et *test* se superposent. Donc le modèle se comportent de la même manière sur les données de *test* que sur les données de *training*. **//// EST CE QUE C'EST VRAI CA ?????? ////**


Traçons maintenant les courbes ROC de tous les modèles de la question 3, pour des prédictions sur les données de test. Cela va nous permettre de comparer les modèles entre eux. On procède de la même manière que précédement pour généré les courbes ROC. Et on stock la sortie de la fonction **prediction** dans le variables **.ROC_pred_test**.
Pour une raison de clarté on appel aussi la fonction sur ModT, même si on l'a déjà fait au début de cette question.

```{r fig.height=5, fig.width=5}
Mod0.ROC_pred_test=prediction(Mod0.predict_test,Y_test)
ModT.ROC_pred_test=prediction(ModT.predict_test,Y_test)
Mod1.ROC_pred_test=prediction(Mod1.predict_test,Y_test)
Mod2.ROC_pred_test=prediction(Mod2.predict_test,Y_test)
ModAIC.ROC_pred_test=prediction(ModAIC.predict_test,Y_test)
```

Pour pouvoir comparer les modèles entre eux il est plus pertinent de comparer les airessous les courbes ROC. Calculons donc ces aires pour nos cinq modèles. Pour ce faire on utilise la fonction **performance** avec l'attribut *auc*. On stock la sortie de cette fonction dans les variables **.perf_AUC**. La valeur de l'aire sous la courbe ROC se trouve alors dans l'attribut *y.values* de ces variable (qui est une liste). On arrondi cette valeur à 3 décimales, et on la stock dans les variables **.AUC**.

```{r fig.height=5, fig.width=5}
Mod0.perf_AUC = performance(Mod0.ROC_pred_test, "auc")
ModT.perf_AUC = performance(ModT.ROC_pred_test, "auc")
Mod1.perf_AUC = performance(Mod1.ROC_pred_test, "auc")
Mod2.perf_AUC = performance(Mod2.ROC_pred_test, "auc")
ModAIC.perf_AUC = performance(ModAIC.ROC_pred_test, "auc")

#Recuperation valeur aire
Mod0.AUC=round(Mod0.perf_AUC@y.values[[1]],3) 
ModT.AUC=round(ModT.perf_AUC@y.values[[1]],3) 
Mod1.AUC=round(Mod1.perf_AUC@y.values[[1]],3) 
Mod2.AUC=round(Mod2.perf_AUC@y.values[[1]],3) 
ModAIC.AUC=round(ModAIC.perf_AUC@y.values[[1]],3)
```

On peut alors tracer un graphique contenant les ROC de nos cinq modèles. On affiche la valeur de l'aire sous la courbe de ROC de chaque modèle dans la légende. 
```{r fig.height=5, fig.width=5}
#Plot tous les ROC
plot(performance(Mod0.ROC_pred_test,"sens","fpr"),col=2,lwd=3,add=FALSE)
plot(performance(ModT.ROC_pred_test,"sens","fpr"),col=3,lwd=3,lty=1,add=TRUE)
plot(performance(Mod1.ROC_pred_test,"sens","fpr"),col=4,lwd=3,lty=2,add=TRUE)
plot(performance(Mod2.ROC_pred_test,"sens","fpr"),col=5,lwd=3,lty=3,add=TRUE)
plot(performance(ModAIC.ROC_pred_test,"sens","fpr"),col=6,lwd=3,lty=4,add=TRUE)
lines(c(0,1),c(0,1),col=1,lty=1,lwd=1)


#Titre et legende avec valeur aire
title("ROC sur les données de test")
legend("bottomright", 
       legend=c(paste("Mod0: ", Mod0.AUC),
                paste("ModT: ", ModT.AUC),
                paste("Mod1: ", Mod1.AUC), 
                paste("Mod2: ", Mod2.AUC), 
                paste("ModAIC: ", ModAIC.AUC)), 
       col=c(2,3,4,5,6), lty=c(1,1,2,3,4))
```





# Partie 2

## Question 1: K-NN

La méthode des k plus proches voisins (knn ou k nearest neighbours) est une méthode simple à utiliser dans des cas de classification ou régression. Il s'agit d'une méthode non paramétrique dans laquelle le modèle mémorise les observations de l'ensemble d'apprentissage et s'en sert pour les observations des données de test. Elle consiste à fixer un nombre k de voisins des nouvelles données d'entrée, de séléctionner les k plus proches (en fonction d'une certaine distance, la distance euclidienne par exemple) et de conserver la classe correspondant à celle majoritairement représentée parmi les différents voisins retenus.

Pour choisir le meilleur k, il faut tester différentes valeurs et retenir celle qui minimise le taux d'erreur de l'ensemble de test.

## Question 2: Implémentation K-NN

```{r}
library(class)
pred_knn_test <-knn(X_training,X_test, Y_training, k=1) # tout d'abord on teste avec k=1
summary(pred_knn_test)
err_test_1=sum(pred_knn_test!=Y_test)/length(Y_test)

err_test = rep(NA,length=150)
err_train = rep(NA, length=150)
for (k in 1:200){
  mod_train <-knn(X_training,X_training,Y_training,k=k)
  pred_knn <-knn(X_training,X_test, Y_training, k=k)
  err_train[k] = mean(mod_train!=Y_training)
  err_test[k] = mean(pred_knn!=Y_test)}

K = which.min(err_test)
K # c'est toujours le cas où k=1 qui minimise le taux d'erreur de test
err_test[(-1)]
vec_k = 1:200
plot(vec_k, err_test, type="b", col="blue", xlab="nombre de voisins",ylab=" erreurs train et test", pch=20, 
     ylim=range(c(err_test, err_train)))
# erreur d'apprentissage
lines(vec_k, err_train,type="b",col="red",pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))

```

